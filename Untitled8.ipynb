{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhXU7+l0a9ko/rl6IrtFul",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananthu191030/RAG/blob/newbranch/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd7zAEbq4O_l",
        "outputId": "62aadcf3-b048-44f8-c6fc-7c758d63d4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (10.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.32.3)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.8.30)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.16.1)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13539 sha256=1572017e3c2621316bd55207dada07c6660d19963fdaed934caa0e6aa11c726e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=7284de8ad896c9a1723302515d72f35bbdd262ce759b5e46695ca1954ccd47d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398379 sha256=c4678769dfd494af693c2c21a00d70f62714cd6d7a642522601ffa4b9004a3b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=84b878771f35d928b2d26645f05e831520e11f2e2e65b829c7b5431ab2c344fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.2\n"
          ]
        }
      ],
      "source": [
        "! pip install newspaper3k\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "\n",
        "def scrape_web(query, max_articles=3):\n",
        "    print(f\"Scraping articles for query: {query}\")\n",
        "\n",
        "    # Example list of news URLs for testing\n",
        "    news_urls = [\n",
        "        f\"https://www.cnn.com/search?q={query.replace(' ', '+')}\",\n",
        "        f\"https://www.bbc.co.uk/search?q={query.replace(' ', '+')}\",\n",
        "        # Add more news sources or search methods as necessary\n",
        "    ]\n",
        "\n",
        "    articles = []\n",
        "\n",
        "    for url in news_urls[:max_articles]:\n",
        "        try:\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            articles.append(article.text)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape {url}: {e}\")\n",
        "\n",
        "    return articles\n"
      ],
      "metadata": {
        "id": "bXIZ5A7L4br7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8coDXQp5As5",
        "outputId": "ee7b54a4-a6e7-49c5-e54e-9f0f97e2db07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4fVQMLH5M5s",
        "outputId": "8a4bb688-6997-4c2b-a4ea-66c55239a173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ5xjZ4t5t9U",
        "outputId": "ab971ab0-df0a-49e1-e932-baad5612ebe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from newspaper import Article\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "\n",
        "# Ensure the model runs on CPU\n",
        "device = torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Load the sentence embedding model\n",
        "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Vectorize and ensure non-empty embeddings\n",
        "def vectorize_text(text_list):\n",
        "    if len(text_list) == 0:\n",
        "        print(\"Text list is empty, no embeddings to generate.\")\n",
        "        return []\n",
        "    return embedder.encode(text_list)\n",
        "\n",
        "def build_faiss_index(documents):\n",
        "    if not documents:\n",
        "        print(\"No documents available to index.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Number of documents: {len(documents)}\")\n",
        "\n",
        "    doc_embeddings = vectorize_text(documents)\n",
        "\n",
        "    if len(doc_embeddings) == 0 or len(doc_embeddings.shape) < 2:\n",
        "        print(\"Failed to generate valid document embeddings.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Generated document embeddings of shape: {doc_embeddings.shape}\")\n",
        "\n",
        "    dimension = doc_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(doc_embeddings)\n",
        "\n",
        "    print(\"FAISS index successfully built.\")\n",
        "\n",
        "    return index\n",
        "\n",
        "def search_faiss_index(index, query, documents, k=3):\n",
        "    if index is None:\n",
        "        print(\"FAISS index is not available, skipping search.\")\n",
        "        return []\n",
        "\n",
        "    query_embedding = vectorize_text([query])\n",
        "    if len(query_embedding) == 0:\n",
        "        print(\"Query embedding failed.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Query embedding shape: {query_embedding.shape}\")\n",
        "\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    print(f\"Found {len(indices[0])} similar documents.\")\n",
        "    return [documents[i] for i in indices[0] if i < len(documents)]\n",
        "\n",
        "def combine_responses(llm_response, retrieved_docs):\n",
        "    combined_input = llm_response + \"\\n\\nRelevant Information:\\n\" + \"\\n\".join(retrieved_docs)\n",
        "    return combined_input\n",
        "\n",
        "def scrape_web(query, max_articles=3):\n",
        "    print(f\"Scraping articles for query: {query}\")\n",
        "\n",
        "    news_urls = [\n",
        "        f\"https://www.cnn.com/search?q={query.replace(' ', '+')}\",\n",
        "        f\"https://www.bbc.co.uk/search?q={query.replace(' ', '+')}\",\n",
        "        # Add more reliable sources if needed\n",
        "    ]\n",
        "\n",
        "    articles = []\n",
        "\n",
        "    for url in news_urls[:max_articles]:\n",
        "        try:\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            articles.append(article.text)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape {url}: {e}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "def process_query(query):\n",
        "    print(f\"Processing query: {query}\")\n",
        "\n",
        "    # Step 1: LLM initial processing with updated model\n",
        "    inputs = tokenizer(query, return_tensors='pt', truncation=True, max_length=512)  # Truncate input if too long\n",
        "    inputs = inputs.to(device)  # Ensure inputs are on the same device as the model\n",
        "\n",
        "    # Generate initial response\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,  # Reduced token count for less verbosity\n",
        "        top_k=50,\n",
        "        temperature=0.3,  # Lower temperature for more coherent outputs\n",
        "        repetition_penalty=2.0  # Increase repetition penalty further\n",
        "    )\n",
        "    llm_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"LLM initial response: {llm_response}\")\n",
        "\n",
        "    # Step 2: Web scraping\n",
        "    documents = scrape_web(query)\n",
        "    print(f\"Scraped {len(documents)} documents.\")\n",
        "\n",
        "    # Step 3: Build FAISS index\n",
        "    faiss_index = build_faiss_index(documents)\n",
        "\n",
        "    # Step 4: Retrieve similar documents\n",
        "    retrieved_docs = search_faiss_index(faiss_index, query, documents)\n",
        "    print(f\"Retrieved {len(retrieved_docs)} similar documents.\")\n",
        "\n",
        "    # Step 5: Combine LLM response and retrieved documents\n",
        "    combined_input = combine_responses(llm_response, retrieved_docs)\n",
        "\n",
        "    # Step 6: Refine with LLM again\n",
        "    refined_inputs = tokenizer(combined_input, return_tensors='pt', truncation=True, max_length=512)  # Truncate if necessary\n",
        "    refined_inputs = refined_inputs.to(device)\n",
        "\n",
        "    # Generate final response\n",
        "    refined_output = model.generate(\n",
        "        **refined_inputs,\n",
        "        max_new_tokens=100,  # Consistent token count\n",
        "        top_k=50,\n",
        "        temperature=0.3,\n",
        "        repetition_penalty=2.0  # Increased repetition penalty\n",
        "    )\n",
        "    final_output = tokenizer.decode(refined_output[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"Final output generated.\")\n",
        "    return final_output\n",
        "\n",
        "# Example query\n",
        "query = \"What are the latest applications of artificial intelligence in healthcare?\"\n",
        "final_result = process_query(query)\n",
        "print(\"Final result:\", final_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szga2pjMACy3",
        "outputId": "2e064719-3686-4d1f-bae3-70622613eb8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing query: What are the latest applications of artificial intelligence in healthcare?\n",
            "LLM initial response: What are the latest applications of artificial intelligence in healthcare?\n",
            "\n",
            "The latest applications of artificial intelligence (AI) in healthcare are mainly focused on the application of machine learning, which is a new field of research. The main focus of AI in healthcare is to improve the quality of care and reduce the costs of healthcare.\n",
            "\n",
            "The main goal of AI in healthcare is to improve the quality of care and reduce the costs of healthcare. The main problem of AI in healthcare is that it is not able to predict the future behavior of patients or their health status.\n",
            "Scraping articles for query: What are the latest applications of artificial intelligence in healthcare?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 2 documents.\n",
            "Number of documents: 2\n",
            "Generated document embeddings of shape: (2, 384)\n",
            "FAISS index successfully built.\n",
            "Query embedding shape: (1, 384)\n",
            "Found 3 similar documents.\n",
            "Retrieved 3 similar documents.\n",
            "Final output generated.\n",
            "Final result: What are the latest applications of artificial intelligence in healthcare?\n",
            "\n",
            "The latest applications of artificial intelligence (AI) in healthcare are mainly focused on the application of machine learning, which is a new field of research. The main focus of AI in healthcare is to improve the quality of care and reduce the costs of healthcare.\n",
            "\n",
            "The main goal of AI in healthcare is to improve the quality of care and reduce the costs of healthcare. The main problem of AI in healthcare is that it is not able to predict the future behavior of patients or their health status.\n",
            "\n",
            "Relevant Information:\n",
            "The Climate Question: Can artificial intelligence help farmers adapt to the effects of climate change? Listen NowThe Climate Question: Can artificial intelligence help farmers adapt to the effects of climate change?\n",
            "Displaying results out of for\n",
            "\n",
            "Your search for did not match any results.\n",
            "\n",
            "A few suggestions:\n",
            "The Climate Question: Can artificial intelligence help farmers adapt to the effects of climate change? Listen NowThe Climate Question: Can artificial intelligence help farmers adapt to the effects of climate change?\n",
            "Displaying results out of for\n",
            "\n",
            "Your search for did not match any results.\n",
            "\n",
            "A few suggestions:\n",
            "The Climate Question: Can artificial intelligence help farmers adapt to the effects of climate change? Listen NowThe Climate Question: Can artificial intelligence help farmers adapt to the effects of climate change?\n",
            "Displaying results out of for\n",
            "\n",
            "Your search for did not match any results.\n",
            "\n",
            "A few suggestions:\n",
            "The Climate Question: Can artificial intelligence help farmers adapt to the effects of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from newspaper import Article\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "\n",
        "# Ensure the model runs on CPU\n",
        "device = torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Load the sentence embedding model\n",
        "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "def vectorize_text(text_list):\n",
        "    if len(text_list) == 0:\n",
        "        print(\"Text list is empty, no embeddings to generate.\")\n",
        "        return []\n",
        "    return embedder.encode(text_list)\n",
        "\n",
        "def build_faiss_index(documents):\n",
        "    if not documents:\n",
        "        print(\"No documents available to index.\")\n",
        "        return None\n",
        "\n",
        "    doc_embeddings = vectorize_text(documents)\n",
        "\n",
        "    if len(doc_embeddings) == 0 or len(doc_embeddings.shape) < 2:\n",
        "        print(\"Failed to generate valid document embeddings.\")\n",
        "        return None\n",
        "\n",
        "    dimension = doc_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(doc_embeddings)\n",
        "\n",
        "    return index\n",
        "\n",
        "def search_faiss_index(index, query, documents, k=3):\n",
        "    if index is None:\n",
        "        print(\"FAISS index is not available, skipping search.\")\n",
        "        return []\n",
        "\n",
        "    query_embedding = vectorize_text([query])\n",
        "    if len(query_embedding) == 0:\n",
        "        print(\"Query embedding failed.\")\n",
        "        return []\n",
        "\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    return [documents[i] for i in indices[0] if i < len(documents)]\n",
        "\n",
        "def combine_responses(llm_response, retrieved_docs):\n",
        "    combined_input = \"Based on the following relevant information, explain: \\n\" + llm_response + \"\\n\\nRelevant Information:\\n\" + \"\\n\".join(retrieved_docs)\n",
        "    return combined_input\n",
        "\n",
        "def scrape_web(query, max_articles=3):\n",
        "    news_urls = [\n",
        "        f\"https://www.cnn.com/search?q={query.replace(' ', '+')}\",\n",
        "        f\"https://www.bbc.co.uk/search?q={query.replace(' ', '+')}\",\n",
        "    ]\n",
        "\n",
        "    articles = []\n",
        "\n",
        "    for url in news_urls[:max_articles]:\n",
        "        try:\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            articles.append(article.text)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape {url}: {e}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "def process_query(query):\n",
        "    print(f\"Processing query: {query}\")\n",
        "\n",
        "    # Step 1: LLM initial processing\n",
        "    inputs = tokenizer(query, return_tensors='pt', truncation=True, max_length=512)\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        top_k=50,\n",
        "        temperature=0.3,\n",
        "        repetition_penalty=2.5  # Increase the repetition penalty\n",
        "    )\n",
        "    llm_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"LLM initial response: {llm_response}\")\n",
        "\n",
        "    # Step 2: Web scraping\n",
        "    documents = scrape_web(query)\n",
        "    print(f\"Scraped {len(documents)} documents.\")\n",
        "\n",
        "    # Step 3: Build FAISS index\n",
        "    faiss_index = build_faiss_index(documents)\n",
        "\n",
        "    # Step 4: Retrieve similar documents\n",
        "    retrieved_docs = search_faiss_index(faiss_index, query, documents)\n",
        "\n",
        "    # Step 5: Combine LLM response and retrieved documents\n",
        "    combined_input = combine_responses(llm_response, retrieved_docs)\n",
        "\n",
        "    # Step 6: Refine with LLM again\n",
        "    refined_inputs = tokenizer(combined_input, return_tensors='pt', truncation=True, max_length=512)\n",
        "    refined_inputs = refined_inputs.to(device)\n",
        "\n",
        "    # Generate final response\n",
        "    refined_output = model.generate(\n",
        "        **refined_inputs,\n",
        "        max_new_tokens=150,\n",
        "        top_k=50,\n",
        "        temperature=0.3,\n",
        "        repetition_penalty=2.5  # Consistent with earlier setting\n",
        "    )\n",
        "    final_output = tokenizer.decode(refined_output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Post-processing to improve coherence\n",
        "    final_output = ' '.join(dict.fromkeys(final_output.split()))  # Remove duplicates\n",
        "\n",
        "    print(\"Final output generated.\")\n",
        "    return final_output\n",
        "\n",
        "# Example query\n",
        "query = \"What are the latest applications of artificial intelligence in healthcare?\"\n",
        "final_result = process_query(query)\n",
        "print(\"Final result:\", final_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceOEHCwzA-hr",
        "outputId": "b44c84ba-cd7e-4428-cd24-ef76ca0a0e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing query: What are the latest applications of artificial intelligence in healthcare?\n",
            "LLM initial response: What are the latest applications of artificial intelligence in healthcare?\n",
            "\n",
            "The latest applications of artificial intelligence (AI) in healthcare have been discussed in the last few years. The most recent applications include artificial neural networks (ANNs), artificial vision systems, and artificial speech recognition.\n",
            "\n",
            "The latest applications of artificial intelligence in healthcare include:\n",
            "\n",
            "A new method for detecting a patient’s heartbeat using a computer-aided diagnosis system (CAD). This method is based on the principle of “the human brain being able to recognize what we see\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 2 documents.\n",
            "Final output generated.\n",
            "Final result: Based on the following relevant information, explain: What are latest applications of artificial intelligence in healthcare? The (AI) healthcare have been discussed last few years. most recent include neural networks (ANNs), vision systems, and speech recognition. include: A new method for detecting a patient’s heartbeat using computer-aided diagnosis system (CAD). This is based principle “the human brain being able to recognize what we see Relevant Information: Climate Question: Can help farmers adapt effects climate change? Listen NowThe Displaying results out Your search did not match any results. suggestions: Question\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from newspaper import Article\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "\n",
        "# Ensure the model runs on CPU\n",
        "device = torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Load the sentence embedding model\n",
        "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "def vectorize_text(text_list):\n",
        "    if len(text_list) == 0:\n",
        "        print(\"Text list is empty, no embeddings to generate.\")\n",
        "        return []\n",
        "    return embedder.encode(text_list)\n",
        "\n",
        "def build_faiss_index(documents):\n",
        "    if not documents:\n",
        "        print(\"No documents available to index.\")\n",
        "        return None\n",
        "\n",
        "    doc_embeddings = vectorize_text(documents)\n",
        "\n",
        "    if len(doc_embeddings) == 0 or len(doc_embeddings.shape) < 2:\n",
        "        print(\"Failed to generate valid document embeddings.\")\n",
        "        return None\n",
        "\n",
        "    dimension = doc_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(doc_embeddings)\n",
        "\n",
        "    return index\n",
        "\n",
        "def search_faiss_index(index, query, documents, k=3):\n",
        "    if index is None:\n",
        "        print(\"FAISS index is not available, skipping search.\")\n",
        "        return []\n",
        "\n",
        "    query_embedding = vectorize_text([query])\n",
        "    if len(query_embedding) == 0:\n",
        "        print(\"Query embedding failed.\")\n",
        "        return []\n",
        "\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    return [documents[i] for i in indices[0] if i < len(documents)]\n",
        "\n",
        "def summarize_documents(documents):\n",
        "    \"\"\" Summarize the documents to improve coherence. \"\"\"\n",
        "    summarized_text = \"\"\n",
        "    for doc in documents:\n",
        "        summarized_text += f\"{doc}\\n\\n\"\n",
        "    return summarized_text\n",
        "\n",
        "def combine_responses(llm_response, retrieved_docs):\n",
        "    summarized_docs = summarize_documents(retrieved_docs)\n",
        "    combined_input = f\"Based on the retrieved information, here are the applications of AI in healthcare:\\n\\n{llm_response}\\n\\nRelevant Information:\\n{summarized_docs}\"\n",
        "    return combined_input\n",
        "\n",
        "def scrape_web(query, max_articles=3):\n",
        "    news_urls = [\n",
        "        f\"https://www.cnn.com/search?q={query.replace(' ', '+')}\",\n",
        "        f\"https://www.bbc.co.uk/search?q={query.replace(' ', '+')}\",\n",
        "        # Add more reliable sources if needed\n",
        "    ]\n",
        "\n",
        "    articles = []\n",
        "\n",
        "    for url in news_urls[:max_articles]:\n",
        "        try:\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            articles.append(article.text)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape {url}: {e}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "def process_query(query):\n",
        "    print(f\"Processing query: {query}\")\n",
        "\n",
        "    # Step 1: LLM initial processing\n",
        "    initial_prompt = f\"Provide a detailed answer about the latest applications of artificial intelligence in healthcare, including specific examples and their impact.\"\n",
        "    inputs = tokenizer(initial_prompt, return_tensors='pt', truncation=True, max_length=512)\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        top_k=50,\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=2.5\n",
        "    )\n",
        "    llm_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"LLM initial response: {llm_response}\")\n",
        "\n",
        "    # Step 2: Web scraping\n",
        "    documents = scrape_web(query)\n",
        "    print(f\"Scraped {len(documents)} documents.\")\n",
        "\n",
        "    # Step 3: Build FAISS index\n",
        "    faiss_index = build_faiss_index(documents)\n",
        "\n",
        "    # Step 4: Retrieve similar documents\n",
        "    retrieved_docs = search_faiss_index(faiss_index, query, documents)\n",
        "\n",
        "    # Step 5: Combine LLM response and retrieved documents\n",
        "    combined_input = combine_responses(llm_response, retrieved_docs)\n",
        "\n",
        "    # Step 6: Refine with LLM again\n",
        "    refined_inputs = tokenizer(combined_input, return_tensors='pt', truncation=True, max_length=512)\n",
        "    refined_inputs = refined_inputs.to(device)\n",
        "\n",
        "    # Generate final response\n",
        "    refined_output = model.generate(\n",
        "        **refined_inputs,\n",
        "        max_new_tokens=150,\n",
        "        top_k=50,\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=2.5\n",
        "    )\n",
        "    final_output = tokenizer.decode(refined_output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Post-processing to improve coherence\n",
        "    final_output = ' '.join(dict.fromkeys(final_output.split()))  # Remove duplicates\n",
        "    final_output = final_output.replace(\"  \", \" \")  # Clean up double spaces\n",
        "\n",
        "    print(\"Final output generated.\")\n",
        "    return final_output\n",
        "\n",
        "# Example query\n",
        "query = \"What are the latest applications of artificial intelligence in healthcare?\"\n",
        "final_result = process_query(query)\n",
        "print(\"Final result:\", final_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laAY1G2zB7iP",
        "outputId": "17d33b22-089e-43e7-f10e-76571e37f702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing query: What are the latest applications of artificial intelligence in healthcare?\n",
            "LLM initial response: Provide a detailed answer about the latest applications of artificial intelligence in healthcare, including specific examples and their impact.\n",
            "\n",
            "The following is an excerpt from the article:\n",
            "\n",
            "“The new technology that will allow doctors to better manage their patients’ health care needs has been developed by researchers at the University of California San Francisco (UCSF) and the National Institutes of Health (NIH). The research team used artificial intelligence to develop a novel artificial intelligence system that can be used to improve patient management for more than 100,000 patients each year.”\n",
            "\n",
            "The article describes the research as follows:\n",
            "\n",
            "“The researchers have shown that using artificial intelligence to help doctors manage their patients’ health care needs can improve outcomes such as quality of life, productivity, and overall health.”\n",
            "\n",
            "The article also explains that �\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 2 documents.\n",
            "Final output generated.\n",
            "Final result: Based on the retrieved information, here are applications of AI in healthcare: Provide a detailed answer about latest artificial intelligence healthcare, including specific examples and their impact. The following is an excerpt from article: “The new technology that will allow doctors to better manage patients’ health care needs has been developed by researchers at University California San Francisco (UCSF) National Institutes Health (NIH). research team used develop novel system can be improve patient management for more than 100,000 patients each year.” article describes as follows: have shown using help outcomes such quality life, productivity, overall health.” also explains � Relevant Information: Climate Question: Can farmers adapt effects climate change? Listen NowThe Displaying results out Your search did not match any results. A few suggestions: Science Questions: How we our through intelligence? Nature questions: What benefits when it comes improving your health? Future Humanities\n"
          ]
        }
      ]
    }
  ]
}